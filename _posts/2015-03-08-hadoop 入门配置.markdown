# Hadoop 
### 1.Configuration

#### Use the following:
- **Prepare to Start the Hadoop Cluster:**
edit the file ***etc/hadoop/hadoop-env.sh*** to define some parameters as follows:
```
# set to the root of your Java installation
export JAVA_HOME=/usr/java/latest
```


- **etc/hadoop/core-site.xml:**
```
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```
- **etc/hadoop/hdfs-site.xml:**
```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```
### 2.Execution
- **Format the filesystem:**
``
$ bin/hdfs namenode -format
``
- **Start NameNode daemon and DataNode daemon:**
``
$ sbin/start-dfs.sh
``

The hadoop daemon log output is written to the $HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs).



----------------------
Hadoop安装完后，启动时报
Error: JAVA_HOME is not set and could not be found.

【解决办法】：修改/etc/hadoop/hadoop-env.sh中设JAVA_HOME。应当使用绝对路径。

export JAVA_HOME=$JAVA_HOME //错误，不能这么改

export JAVA_HOME=/usr/java/jdk1.6.0_45 //正确，应该这么改